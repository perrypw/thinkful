{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Keyword Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.637\n",
      "Sensitivity: 0.316\n",
      "Specificity: 0.958\n",
      "Improvement over baseline: 0.137\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Guessed Neg</th>\n",
       "      <th>Guessed Pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Neg</th>\n",
       "      <td>479</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pos</th>\n",
       "      <td>342</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Guessed Neg  Guessed Pos\n",
       "Neg          479           21\n",
       "Pos          342          158"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "amazon = pd.read_table(\"amazon.txt\",header=None)\n",
    "yelp = pd.read_table(\"yelp.txt\")\n",
    "\n",
    "amazon.columns = ['text','positive']\n",
    "yelp.columns = ['text','positive']\n",
    "\n",
    "# Make outcome column boolean\n",
    "amazon['positive'] = amazon['positive'] == 1\n",
    "\n",
    "\n",
    "# Positive and negative keywords\n",
    "positive_words = ['awesome', 'superb','perfect','enjoyable','outstanding', 'fantastic', 'terrific', 'good', 'nice', 'great',\n",
    "                 'amazing','love','loved','enjoy','nice','awesome','best','super']\n",
    "negative_words = ['bad', 'terrible','useless', 'hate', 'sucks','worst','awful','ineffective','lame','stupid','cheap','unreliable',\n",
    "                 'fail','poor','broke','broken',\"doesn't work\"]\n",
    "\n",
    "# Making the df column names for each variable\n",
    "#for word in positive_words + negative_words:\n",
    "   #amazon[word] = amazon['text'].str.contains(' ' + word + ' ',case=False) | amazon['text'].str.lower().str.split(' ').str.get(0).str.contains(word)\n",
    "\n",
    "for word in positive_words + negative_words:\n",
    "    #amazon[word] = amazon['text'].apply(lambda x: [w in x for w in positive_words + negative_words])\n",
    "    amazon[word] = amazon['text'].apply(lambda x: word in x)\n",
    "    \n",
    "# Set up the outcome variable and model vars\n",
    "variables = amazon[positive_words + negative_words]\n",
    "outcome = amazon['positive']\n",
    "\n",
    "# Import model, instantiate, and train\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "bernoulli = BernoulliNB()\n",
    "bernoulli.fit(variables, outcome)\n",
    "\n",
    "# Store predictions\n",
    "model1predictions = bernoulli.predict(variables)\n",
    "\n",
    "# Add array for whether or not prediction was accurate for said observation\n",
    "model1results =  model1predictions == amazon['positive']\n",
    "model1_accuracy = 1- (model1results.value_counts(dropna=False).loc[False] / len(model1results))\n",
    "#model1_sensitivity\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "c = confusion_matrix(outcome, model1predictions)\n",
    "confusion = pd.DataFrame(c)\n",
    "confusion = confusion.rename(columns={0:'Guessed Neg',1:'Guessed Pos'},index={0:'Neg',1:'Pos'})\n",
    "sensitivity = confusion.loc['Pos','Guessed Pos'] / confusion.sum(axis=1).loc['Pos']\n",
    "specificity = confusion.loc['Neg','Guessed Neg'] / confusion.sum(axis=1).loc['Neg']\n",
    "\n",
    "print('Accuracy: {}'.format(model1_accuracy))\n",
    "print('Sensitivity: {}'.format(sensitivity))\n",
    "print('Specificity: {}'.format(specificity))\n",
    "print('Improvement over baseline: {}'.format(model1_accuracy-0.5))\n",
    "confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessment\n",
    "\n",
    "The overall accuracy is 0.637, with a specificity of 0.96 and sensitivity of 0.32. Interestingly, this simplified model is very good at predicting negative reviews but pretty terrible at guessing positive reviews. I don't think it's accurate enough to bother using on the other data considering the other versions we have available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common words in pos/neg reviews model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "amazon = pd.read_table(\"amazon.txt\",header=None)\n",
    "yelp = pd.read_table(\"yelp.txt\",header=None)\n",
    "\n",
    "amazon.columns = ['text','positive']\n",
    "yelp.columns = ['text','positive']\n",
    "\n",
    "# Make outcome column boolean\n",
    "amazon['positive'] = amazon['positive'] == 1\n",
    "yelp['positive'] = yelp['positive'] == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Converting all text columns into a list of words\n",
    "amazon['text'] = np.array(amazon['text'].str.lower().str.split(\" |,|\\.|!|\\?\"))\n",
    "\n",
    "# Creating positive and negative subsets to calculate most popular words for pos & neg\n",
    "amazon_pos = amazon[amazon['positive'] == 1]\n",
    "amazon_neg = amazon[amazon['positive'] == 0]\n",
    "\n",
    "# Calculate most common words in positive & negative reviews\n",
    "x = sum(amazon_pos['text'], [])\n",
    "y = sum(amazon_neg['text'], [])\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "for item in x:\n",
    "    c = Counter(x)\n",
    "    common_tuples = c.most_common(200)\n",
    "    top_pos_words = [i[0] for i in common_tuples]\n",
    "\n",
    "for item in y:\n",
    "    c = Counter(y)\n",
    "    common_tuples = c.most_common(200)\n",
    "    top_neg_words = [i[0] for i in common_tuples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove overlapping words for each\n",
    "pos_unique = [x for x in top_pos_words if x not in top_neg_words]\n",
    "neg_unique = [x for x in top_neg_words if x not in top_pos_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74 74\n"
     ]
    }
   ],
   "source": [
    "print(len(pos_unique),len(neg_unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['works', 'excellent', 'price', 'best', 'nice', 'love', 'easy', 'comfortable', 'happy', 'fine', 'been', 'far', 'clear', 'device', 'cell', 'fits', 'camera', 'got', 'working', 'highly', 'pretty', 'years', 'without', 'everything', 'cool', 'wear', 'lot', 'jabra', 'people', 'found', 'both', 'light', 'perfectly', 'value', 'impressed', 'say', 'priced', 'sturdy', 'gets', 'little', 'tried', 'definitely', 'pleased', 'small', 'voice', 'awesome', 'overall', 'range', 'amazon', 'cases', 'original', 'ears', 'seems', 'keyboard', 'their', 'several', 'most', 'headsets', 'verizon', 'order', 'free', 'shipping', 'pictures', 'leather', 'fast', 'comfortably', 'job', '&', 'glad', 'phones', 'look', 'charm', 'being', 'simple']\n"
     ]
    }
   ],
   "source": [
    "print(pos_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['money', 'first', 'then', 'do', 'poor', 'waste', \"doesn't\", 'bad', 'what', 'could', 'worst', 'will', 'calls', 'off', 'same', 'piece', 'hear', 'charge', 'disappointed', 'enough', 'thing', 'terrible', 'plug', 'volume', 'design', 'horrible', 'customer', 'junk', 'unit', 'by', 'how', \"didn't\", 'talk', 'broke', 'over', 'useless', 'back', 'however', 'last', 'went', '3', 'days', 'buttons', 'months', 'completely', 'stay', 'company', 'never', 'crap', 'difficult', 'cheap', 'way', 'dropped', 'we', 'big', 'week', 'within', 'down', '1', 'signal', 'put', 'some', 'disappointment', 'return', 'old', 'nokia', 'want', 'anything', 'disappointing', 'picture', 'low', 'anyone', 'none', 'easily']\n"
     ]
    }
   ],
   "source": [
    "print(neg_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      [so, there, is, no, way, for, me, to, plug, it...\n",
      "1                     [good, case, , excellent, value, ]\n",
      "2                           [great, for, the, jawbone, ]\n",
      "3      [tied, to, charger, for, conversations, lastin...\n",
      "4                                [the, mic, is, great, ]\n",
      "5      [i, have, to, jiggle, the, plug, to, get, it, ...\n",
      "6      [if, you, have, several, dozen, or, several, h...\n",
      "7      [if, you, are, razr, owner, , , you, must, hav...\n",
      "8          [needless, to, say, , i, wasted, my, money, ]\n",
      "9             [what, a, waste, of, money, and, time, , ]\n",
      "10               [and, the, sound, quality, is, great, ]\n",
      "11     [he, was, very, impressed, when, going, from, ...\n",
      "12     [if, the, two, were, seperated, by, a, mere, 5...\n",
      "13                         [very, good, quality, though]\n",
      "14     [the, design, is, very, odd, , as, the, ear, \"...\n",
      "15     [highly, recommend, for, any, one, who, has, a...\n",
      "16          [i, advise, everyone, do, not, be, fooled, ]\n",
      "17                               [so, far, so, good, , ]\n",
      "18                                    [works, great, , ]\n",
      "19     [it, clicks, into, place, in, a, way, that, ma...\n",
      "20     [i, went, on, motorola's, website, and, follow...\n",
      "21     [i, bought, this, to, use, with, my, kindle, f...\n",
      "22      [the, commercials, are, the, most, misleading, ]\n",
      "23     [i, have, yet, to, run, this, new, battery, be...\n",
      "24     [i, bought, it, for, my, mother, and, she, had...\n",
      "25          [great, pocket, pc, /, phone, combination, ]\n",
      "26     [i've, owned, this, phone, for, 7, months, now...\n",
      "27     [i, didn't, think, that, the, instructions, pr...\n",
      "28     [people, couldnt, hear, me, talk, and, i, had,...\n",
      "29                             [doesn't, hold, charge, ]\n",
      "                             ...                        \n",
      "970    [i, plugged, it, in, only, to, find, out, not,...\n",
      "971                               [excellent, product, ]\n",
      "972                    [earbud, piece, breaks, easily, ]\n",
      "973                                   [lousy, product, ]\n",
      "974    [this, phone, tries, very, hard, to, do, every...\n",
      "975    [it, is, the, best, charger, i, have, seen, on...\n",
      "976                              [sweetest, phone, , , ]\n",
      "977     [:-)oh, , the, charger, seems, to, work, fine, ]\n",
      "978    [it, fits, so, securely, that, the, ear, hook,...\n",
      "979                              [not, enough, volume, ]\n",
      "980          [echo, problem, , , , very, unsatisfactory]\n",
      "981    [you, could, only, take, 2, videos, at, a, tim...\n",
      "982                        [don't, waste, your, money, ]\n",
      "983    [i, am, going, to, have, to, be, the, first, t...\n",
      "984    [adapter, does, not, provide, enough, charging...\n",
      "985    [there, was, so, much, hype, over, this, phone...\n",
      "986    [you, also, cannot, take, pictures, with, it, ...\n",
      "987                        [phone, falls, out, easily, ]\n",
      "988    [it, didn't, work, , people, can, not, hear, m...\n",
      "989    [the, text, messaging, feature, is, really, tr...\n",
      "990    [i'm, really, disappointed, all, i, have, now,...\n",
      "991                            [painful, on, the, ear, ]\n",
      "992            [lasted, one, day, and, then, blew, up, ]\n",
      "993                                     [disappointed, ]\n",
      "994                          [kind, of, flops, around, ]\n",
      "995    [the, screen, does, get, smudged, easily, beca...\n",
      "996    [what, a, piece, of, junk, , , i, lose, more, ...\n",
      "997                  [item, does, not, match, picture, ]\n",
      "998    [the, only, thing, that, disappoint, me, is, t...\n",
      "999    [you, can, not, answer, calls, with, the, unit...\n",
      "Name: text, Length: 1000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(amazon['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8140000000000001\n",
      "Sensitivity: 0.94\n",
      "Specificity: 0.688\n",
      "Improvement over baseline: 0.31400000000000006\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Guessed Neg</th>\n",
       "      <th>Guessed Pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Neg</th>\n",
       "      <td>344</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pos</th>\n",
       "      <td>30</td>\n",
       "      <td>470</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Guessed Neg  Guessed Pos\n",
       "Neg          344          156\n",
       "Pos           30          470"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the features based on our words\n",
    "for word in pos_unique + neg_unique:\n",
    "    amazon[word] = amazon['text'].apply(lambda x: word in x)\n",
    "    yelp[word] = yelp['text'].apply(lambda x: word in x) #Creating the necessary feature for Yelp to use down the road\n",
    "    \n",
    "# Set up the outcome variable and model vars\n",
    "variables = amazon[pos_unique + neg_unique]\n",
    "outcome = amazon['positive']\n",
    "\n",
    "# Import model, instantiate, and train\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "bernoulli = BernoulliNB()\n",
    "bernoulli.fit(variables, outcome)\n",
    "\n",
    "# Store predictions\n",
    "model2predictions = bernoulli.predict(variables)\n",
    "\n",
    "# Add array for whether or not prediction was accurate for said observation\n",
    "model2results =  model2predictions == amazon['positive']\n",
    "model2_accuracy = 1- (model2results.value_counts(dropna=False).loc[False] / len(model2results))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "c = confusion_matrix(outcome, model2predictions)\n",
    "confusion = pd.DataFrame(c)\n",
    "confusion = confusion.rename(columns={0:'Guessed Neg',1:'Guessed Pos'},index={0:'Neg',1:'Pos'})\n",
    "sensitivity = confusion.loc['Pos','Guessed Pos'] / confusion.sum(axis=1).loc['Pos']\n",
    "specificity = confusion.loc['Neg','Guessed Neg'] / confusion.sum(axis=1).loc['Neg']\n",
    "\n",
    "print('Accuracy: {}'.format(model2_accuracy))\n",
    "print('Sensitivity: {}'.format(sensitivity))\n",
    "print('Specificity: {}'.format(specificity))\n",
    "print('Improvement over baseline: {}'.format(model2_accuracy-0.5))\n",
    "confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Assessment\n",
    "\n",
    "This one does better overall and much better in sensitivity. The specificity actually declined a bit with this model, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.639\n",
      "Sensitivity: 0.698\n",
      "Specificity: 0.58\n",
      "Improvement over baseline: 0.139\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Guessed Neg</th>\n",
       "      <th>Guessed Pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Neg</th>\n",
       "      <td>290</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pos</th>\n",
       "      <td>151</td>\n",
       "      <td>349</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Guessed Neg  Guessed Pos\n",
       "Neg          290          210\n",
       "Pos          151          349"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up the outcome variable and model vars\n",
    "yelp_vars = yelp[pos_unique + neg_unique]\n",
    "yelp_actual = yelp['positive']\n",
    "\n",
    "# Store predictions, add as a df column\n",
    "yelp_predictions = bernoulli.predict(yelp_vars)\n",
    "\n",
    "# Add array for whether or not prediction was accurate for said observation\n",
    "model2results =  yelp_predictions == yelp_actual\n",
    "model2_accuracy = 1- (model2results.value_counts(dropna=False).loc[False] / len(model2results))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "c = confusion_matrix(yelp_actual, yelp_predictions)\n",
    "confusion = pd.DataFrame(c)\n",
    "confusion = confusion.rename(columns={0:'Guessed Neg',1:'Guessed Pos'},index={0:'Neg',1:'Pos'})\n",
    "sensitivity = confusion.loc['Pos','Guessed Pos'] / confusion.sum(axis=1).loc['Pos']\n",
    "specificity = confusion.loc['Neg','Guessed Neg'] / confusion.sum(axis=1).loc['Neg']\n",
    "\n",
    "print('Accuracy: {}'.format(model2_accuracy))\n",
    "print('Sensitivity: {}'.format(sensitivity))\n",
    "print('Specificity: {}'.format(specificity))\n",
    "print('Improvement over baseline: {}'.format(model2_accuracy-0.5))\n",
    "confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above model is less accurate, but similar to when we tested results on the Amazon data, it is more sensitive than it is specific. It may be overfitting, so let's try to see what happens if we use a holdout group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7166666666666667\n",
      "Sensitivity: 0.577922077922078\n",
      "Specificity: 0.863013698630137\n",
      "Improvement over baseline: 0.21666666666666667\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Guessed Neg</th>\n",
       "      <th>Guessed Pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Neg</th>\n",
       "      <td>126</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pos</th>\n",
       "      <td>65</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Guessed Neg  Guessed Pos\n",
       "Neg          126           20\n",
       "Pos           65           89"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set up traindata, testdata\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(amazon,test_size=0.3) # Fine tune this\n",
    "traindata, testdata = train[pos_unique + neg_unique], test[pos_unique + neg_unique]\n",
    "\n",
    "# Set up the outcome variable and model vars\n",
    "variables = train[pos_unique + neg_unique]\n",
    "outcome = train['positive']\n",
    "\n",
    "# Import model, instantiate, and train\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "bernoulli = BernoulliNB()\n",
    "bernoulli.fit(variables, outcome)\n",
    "\n",
    "# Store predictions\n",
    "predictions = bernoulli.predict(testdata)\n",
    "\n",
    "# Add array for whether or not prediction was accurate for said observation\n",
    "results =  predictions == test['positive']\n",
    "accuracy = 1- (results.value_counts(dropna=False).loc[False] / len(results))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "c = confusion_matrix(test['positive'], predictions)\n",
    "confusion = pd.DataFrame(c)\n",
    "confusion = confusion.rename(columns={0:'Guessed Neg',1:'Guessed Pos'},index={0:'Neg',1:'Pos'})\n",
    "sensitivity = confusion.loc['Pos','Guessed Pos'] / confusion.sum(axis=1).loc['Pos']\n",
    "specificity = confusion.loc['Neg','Guessed Neg'] / confusion.sum(axis=1).loc['Neg']\n",
    "\n",
    "print('Accuracy: {}'.format(accuracy))\n",
    "print('Sensitivity: {}'.format(sensitivity))\n",
    "print('Specificity: {}'.format(specificity))\n",
    "print('Improvement over baseline: {}'.format(accuracy-0.5))\n",
    "confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy does go down a little bit, so perhaps it was overfit. Let's try this on Yelp data now to see if it works better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.562\n",
      "Sensitivity: 0.318\n",
      "Specificity: 0.806\n",
      "Improvement over baseline: 0.062000000000000055\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Guessed Neg</th>\n",
       "      <th>Guessed Pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Neg</th>\n",
       "      <td>403</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pos</th>\n",
       "      <td>341</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Guessed Neg  Guessed Pos\n",
       "Neg          403           97\n",
       "Pos          341          159"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up the outcome variable and model vars\n",
    "yelp_vars = yelp[pos_unique + neg_unique]\n",
    "yelp_actual = yelp['positive']\n",
    "\n",
    "# Store predictions, add as a df column\n",
    "yelp_predictions = bernoulli.predict(yelp_vars)\n",
    "\n",
    "# Add array for whether or not prediction was accurate for said observation\n",
    "results =  yelp_predictions == yelp_actual\n",
    "accuracy = 1- (results.value_counts(dropna=False).loc[False] / len(results))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "c = confusion_matrix(yelp_actual, yelp_predictions)\n",
    "confusion = pd.DataFrame(c)\n",
    "confusion = confusion.rename(columns={0:'Guessed Neg',1:'Guessed Pos'},index={0:'Neg',1:'Pos'})\n",
    "sensitivity = confusion.loc['Pos','Guessed Pos'] / confusion.sum(axis=1).loc['Pos']\n",
    "specificity = confusion.loc['Neg','Guessed Neg'] / confusion.sum(axis=1).loc['Neg']\n",
    "\n",
    "print('Accuracy: {}'.format(accuracy))\n",
    "print('Sensitivity: {}'.format(sensitivity))\n",
    "print('Specificity: {}'.format(specificity))\n",
    "print('Improvement over baseline: {}'.format(accuracy-0.5))\n",
    "confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we suspected, it doesn't perform so well. Let's try a different type of model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Most Correlated Words as Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "amazon = pd.read_table(\"amazon.txt\",header=None)\n",
    "yelp = pd.read_table(\"yelp.txt\",header=None)\n",
    "\n",
    "amazon.columns = ['text','positive']\n",
    "yelp.columns = ['text','positive']\n",
    "\n",
    "# Make outcome column boolean\n",
    "amazon['positive'] = amazon['positive'] == 1\n",
    "yelp['positive'] = yelp['positive'] == 1\n",
    "\n",
    "# Converting all text columns into a list of words\n",
    "amazon['text'] = np.array(amazon['text'].str.lower().str.split(\" |,|\\.|!|\\?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_words = sum(amazon['text'], [])\n",
    "wordset = set(all_words)\n",
    "wordsdf=pd.DataFrame()\n",
    "\n",
    "# Create df with all of the correlations to positive, by word\n",
    "for word in wordset:\n",
    "    wordsdf[word] = amazon['text'].apply(lambda x: word in x)\n",
    "\n",
    "wordsdf['positive'] = amazon['positive']\n",
    "\n",
    "#calculate correlations - for loop could be more efficient\n",
    "correlations = wordsdf.corr().filter(['positive']).drop(['positive'])\n",
    "pos_correlations = correlations.abs().sort_values('positive',ascending=False)\n",
    "\n",
    "#old, inefficient way\n",
    "#correlations = wordsdf.corr()\n",
    "#pos_correlations = correlations.abs().sort_values('positive',ascending=False).loc[:,'positive']\n",
    "\n",
    "#let's take the top 200\n",
    "features = pos_correlations[:199].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.881\n",
      "Sensitivity: 0.88\n",
      "Specificity: 0.882\n",
      "Improvement over baseline: 0.381\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Guessed Neg</th>\n",
       "      <th>Guessed Pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Neg</th>\n",
       "      <td>441</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pos</th>\n",
       "      <td>60</td>\n",
       "      <td>440</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Guessed Neg  Guessed Pos\n",
       "Neg          441           59\n",
       "Pos           60          440"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add these words as features to the df\n",
    "for word in features:\n",
    "    amazon[word] = amazon['text'].apply(lambda x: word in x)\n",
    "    yelp[word] = yelp['text'].apply(lambda x: word in x) #Creating the necessary feature for Yelp to use down the road\n",
    "    \n",
    "# Set up the outcome variable and model vars\n",
    "variables = amazon[features]\n",
    "outcome = amazon['positive']\n",
    "\n",
    "# Import model, instantiate, and train\n",
    "bernoulli.fit(variables, outcome)\n",
    "\n",
    "# Store predictions, add as a df column\n",
    "predictions = bernoulli.predict(variables)\n",
    "\n",
    "# Add array for whether or not prediction was accurate for said observation\n",
    "results =  predictions == amazon['positive']\n",
    "accuracy = 1- (results.value_counts(dropna=False).loc[False] / len(results))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "c = confusion_matrix(amazon['positive'], predictions)\n",
    "confusion = pd.DataFrame(c)\n",
    "confusion = confusion.rename(columns={0:'Guessed Neg',1:'Guessed Pos'},index={0:'Neg',1:'Pos'})\n",
    "sensitivity = confusion.loc['Pos','Guessed Pos'] / confusion.sum(axis=1).loc['Pos']\n",
    "specificity = confusion.loc['Neg','Guessed Neg'] / confusion.sum(axis=1).loc['Neg']\n",
    "\n",
    "print('Accuracy: {}'.format(accuracy))\n",
    "print('Sensitivity: {}'.format(sensitivity))\n",
    "print('Specificity: {}'.format(specificity))\n",
    "print('Improvement over baseline: {}'.format(accuracy-0.5))\n",
    "confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the best model and methodology we've come up with so far. Now let's see how well it did with Yelp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6779999999999999\n",
      "Sensitivity: 0.51\n",
      "Specificity: 0.846\n",
      "Improvement over baseline: 0.17799999999999994\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Guessed Neg</th>\n",
       "      <th>Guessed Pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Neg</th>\n",
       "      <td>423</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pos</th>\n",
       "      <td>245</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Guessed Neg  Guessed Pos\n",
       "Neg          423           77\n",
       "Pos          245          255"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up the outcome variable and model vars\n",
    "yelp_vars = yelp[features]\n",
    "yelp_actual = yelp['positive']\n",
    "\n",
    "# Store predictions, add as a df column\n",
    "yelp_predictions = bernoulli.predict(yelp_vars)\n",
    "\n",
    "# Add array for whether or not prediction was accurate for said observation\n",
    "results =  yelp_predictions == yelp_actual\n",
    "accuracy = 1- (results.value_counts(dropna=False).loc[False] / len(results))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "c = confusion_matrix(yelp_actual, yelp_predictions)\n",
    "confusion = pd.DataFrame(c)\n",
    "confusion = confusion.rename(columns={0:'Guessed Neg',1:'Guessed Pos'},index={0:'Neg',1:'Pos'})\n",
    "sensitivity = confusion.loc['Pos','Guessed Pos'] / confusion.sum(axis=1).loc['Pos']\n",
    "specificity = confusion.loc['Neg','Guessed Neg'] / confusion.sum(axis=1).loc['Neg']\n",
    "\n",
    "print('Accuracy: {}'.format(accuracy))\n",
    "print('Sensitivity: {}'.format(sensitivity))\n",
    "print('Specificity: {}'.format(specificity))\n",
    "print('Improvement over baseline: {}'.format(accuracy-0.5))\n",
    "confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we may be overfitting again. Let's reduce the number of features and see how we do.\n",
    "\n",
    "# Model 4 - Optimized Features Based on Holdouts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try some optimization of the number of features to try to improve results. We'll try a bunch of different variations of the number of features and see how we do. More specifically, let's try steps of 5 between 0 and 200 (that's about the upper limit of what my computer seems to be able to handle in a reasonable amount of time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline improvement for 4 features: 0.16999999999999993\n",
      "Baseline improvement for 9 features: 0.22666666666666668\n",
      "Baseline improvement for 14 features: 0.2333333333333334\n",
      "Baseline improvement for 19 features: 0.24\n",
      "Baseline improvement for 24 features: 0.22999999999999998\n",
      "Baseline improvement for 29 features: 0.24\n",
      "Baseline improvement for 34 features: 0.24\n",
      "Baseline improvement for 39 features: 0.2466666666666666\n",
      "Baseline improvement for 44 features: 0.2466666666666666\n",
      "Baseline improvement for 49 features: 0.2433333333333334\n",
      "Baseline improvement for 54 features: 0.25\n",
      "Baseline improvement for 59 features: 0.18666666666666665\n",
      "Baseline improvement for 64 features: 0.19666666666666666\n",
      "Baseline improvement for 69 features: 0.19999999999999996\n",
      "Baseline improvement for 74 features: 0.20333333333333337\n",
      "Baseline improvement for 79 features: 0.20666666666666667\n",
      "Baseline improvement for 84 features: 0.19999999999999996\n",
      "Baseline improvement for 89 features: 0.20666666666666667\n",
      "Baseline improvement for 94 features: 0.20666666666666667\n",
      "Baseline improvement for 99 features: 0.20333333333333337\n",
      "Baseline improvement for 104 features: 0.20666666666666667\n",
      "Baseline improvement for 109 features: 0.21333333333333337\n",
      "Baseline improvement for 114 features: 0.22333333333333338\n",
      "Baseline improvement for 119 features: 0.22333333333333338\n",
      "Baseline improvement for 124 features: 0.2333333333333334\n",
      "Baseline improvement for 129 features: 0.22666666666666668\n",
      "Baseline improvement for 134 features: 0.22666666666666668\n",
      "Baseline improvement for 139 features: 0.22666666666666668\n",
      "Baseline improvement for 144 features: 0.22999999999999998\n",
      "Baseline improvement for 149 features: 0.2333333333333334\n",
      "Baseline improvement for 154 features: 0.22999999999999998\n",
      "Baseline improvement for 159 features: 0.2333333333333334\n",
      "Baseline improvement for 164 features: 0.2433333333333334\n",
      "Baseline improvement for 169 features: 0.22999999999999998\n",
      "Baseline improvement for 174 features: 0.2333333333333334\n",
      "Baseline improvement for 179 features: 0.2433333333333334\n",
      "Baseline improvement for 184 features: 0.24\n",
      "Baseline improvement for 189 features: 0.24\n",
      "Baseline improvement for 194 features: 0.2366666666666667\n"
     ]
    }
   ],
   "source": [
    "amazon = pd.read_table(\"amazon.txt\",header=None)\n",
    "yelp = pd.read_table(\"yelp.txt\",header=None)\n",
    "\n",
    "amazon.columns = ['text','positive']\n",
    "yelp.columns = ['text','positive']\n",
    "\n",
    "# Make outcome column boolean\n",
    "amazon['positive'] = amazon['positive'] == 1\n",
    "yelp['positive'] = yelp['positive'] == 1\n",
    "\n",
    "# Converting all text columns into a list of words\n",
    "amazon['text'] = np.array(amazon['text'].str.lower().str.split(\" |,|\\.|!|\\?\"))\n",
    "\n",
    "#set up traindata, testdata\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(amazon,test_size=0.3) # Fine tune this\n",
    "\n",
    "all_words = sum(train['text'], [])\n",
    "wordset = set(all_words)\n",
    "wordsdf=pd.DataFrame()\n",
    "\n",
    "# Create df with all of the correlations to positive, by word\n",
    "for word in wordset:\n",
    "    wordsdf[word] = train['text'].apply(lambda x: word in x)    \n",
    "\n",
    "wordsdf['positive'] = train['positive']\n",
    "\n",
    "#calculate correlations\n",
    "correlations = wordsdf.corr().filter(['positive']).drop(['positive'])\n",
    "pos_correlations = correlations.abs().sort_values('positive',ascending=False)\n",
    "\n",
    "#let's take the top 200\n",
    "features_tot = pos_correlations[:199].index.values\n",
    "\n",
    "for num_features in np.arange(4,199,5):\n",
    "    \n",
    "    features = features_tot[0:num_features]\n",
    "    \n",
    "    # Add these words as features to the df\n",
    "    for word in features:\n",
    "        train[word] = train['text'].apply(lambda x: word in x)\n",
    "        test[word] = test['text'].apply(lambda x: word in x)\n",
    "        yelp[word] = yelp['text'].apply(lambda x: word in x) #Creating the necessary feature for Yelp to use down the road\n",
    "    \n",
    "    traindata, testdata = train[features], test[features]\n",
    "\n",
    "    # Set up the outcome variable and model vars\n",
    "    variables = train[features]\n",
    "    outcome = train['positive']\n",
    "\n",
    "    # Import model, instantiate, and train\n",
    "    bernoulli.fit(variables, outcome)\n",
    "\n",
    "    # Store predictions, add as a df column\n",
    "    predictions = bernoulli.predict(testdata)\n",
    "\n",
    "    # Add array for whether or not prediction was accurate for said observation\n",
    "    results =  predictions == test['positive']\n",
    "    accuracy = 1- (results.value_counts(dropna=False).loc[False] / len(results))\n",
    "    baseline = accuracy-0.5\n",
    "\n",
    "    print('Baseline improvement on test set for {} features: {}'.format(num_features,baseline))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we see declining returns in baseline improvement after 55 features, so let's create a model with that and see how that works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8\n",
      "Sensitivity: 0.736\n",
      "Specificity: 0.864\n",
      "Improvement over baseline: 0.30000000000000004\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Guessed Neg</th>\n",
       "      <th>Guessed Pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Neg</th>\n",
       "      <td>432</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pos</th>\n",
       "      <td>132</td>\n",
       "      <td>368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Guessed Neg  Guessed Pos\n",
       "Neg          432           68\n",
       "Pos          132          368"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 55 features \n",
    "features = pos_correlations[:54].index.values\n",
    "\n",
    "# Add these words as features to the df\n",
    "for word in features:\n",
    "    amazon[word] = amazon['text'].apply(lambda x: word in x)\n",
    "    yelp[word] = yelp['text'].apply(lambda x: word in x) #Creating the necessary feature for Yelp to use down the road\n",
    "    \n",
    "# Set up the outcome variable and model vars\n",
    "variables = amazon[features]\n",
    "outcome = amazon['positive']\n",
    "\n",
    "# Import model, instantiate, and train\n",
    "bernoulli.fit(variables, outcome)\n",
    "\n",
    "# Store predictions, add as a df column\n",
    "predictions = bernoulli.predict(variables)\n",
    "\n",
    "# Add array for whether or not prediction was accurate for said observation\n",
    "results =  predictions == amazon['positive']\n",
    "accuracy = 1- (results.value_counts(dropna=False).loc[False] / len(results))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "c = confusion_matrix(amazon['positive'], predictions)\n",
    "confusion = pd.DataFrame(c)\n",
    "confusion = confusion.rename(columns={0:'Guessed Neg',1:'Guessed Pos'},index={0:'Neg',1:'Pos'})\n",
    "sensitivity = confusion.loc['Pos','Guessed Pos'] / confusion.sum(axis=1).loc['Pos']\n",
    "specificity = confusion.loc['Neg','Guessed Neg'] / confusion.sum(axis=1).loc['Neg']\n",
    "\n",
    "print('Accuracy: {}'.format(accuracy))\n",
    "print('Sensitivity: {}'.format(sensitivity))\n",
    "print('Specificity: {}'.format(specificity))\n",
    "print('Improvement over baseline: {}'.format(accuracy-0.5))\n",
    "confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this one is slightly less accurate than the one with 200 variables, as we would suspect, but interestingly has a noticeably better sensitivity. Let's see how it does versus the Yelp data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6579999999999999\n",
      "Sensitivity: 0.49\n",
      "Specificity: 0.826\n",
      "Improvement over baseline: 0.15799999999999992\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Guessed Neg</th>\n",
       "      <th>Guessed Pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Neg</th>\n",
       "      <td>413</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pos</th>\n",
       "      <td>255</td>\n",
       "      <td>245</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Guessed Neg  Guessed Pos\n",
       "Neg          413           87\n",
       "Pos          255          245"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up the outcome variable and model vars\n",
    "yelp_vars = yelp[features]\n",
    "yelp_actual = yelp['positive']\n",
    "\n",
    "# Store predictions, add as a df column\n",
    "yelp_predictions = bernoulli.predict(yelp_vars)\n",
    "\n",
    "# Add array for whether or not prediction was accurate for said observation\n",
    "results =  yelp_predictions == yelp_actual\n",
    "accuracy = 1- (results.value_counts(dropna=False).loc[False] / len(results))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "c = confusion_matrix(yelp_actual, yelp_predictions)\n",
    "confusion = pd.DataFrame(c)\n",
    "confusion = confusion.rename(columns={0:'Guessed Neg',1:'Guessed Pos'},index={0:'Neg',1:'Pos'})\n",
    "sensitivity = confusion.loc['Pos','Guessed Pos'] / confusion.sum(axis=1).loc['Pos']\n",
    "specificity = confusion.loc['Neg','Guessed Neg'] / confusion.sum(axis=1).loc['Neg']\n",
    "\n",
    "print('Accuracy: {}'.format(accuracy))\n",
    "print('Sensitivity: {}'.format(sensitivity))\n",
    "print('Specificity: {}'.format(specificity))\n",
    "print('Improvement over baseline: {}'.format(accuracy-0.5))\n",
    "confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It performs about as well on the Yelp data. At this point, I think it's safe to say that our methodology has us stuck in about a 65-70% range for accuracy on the Yelp data, and modifications of the number of features may not be enough to breka out of that. Just for fun, let's try a PCA on those 200 features to see how that performs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
