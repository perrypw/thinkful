{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer, VectorAssembler\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "from pyspark.sql.functions import isnan, when, count, col, concat, lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our constants\n",
    "DATA_NAME = 'reviews_Musical_Instruments_5.json'\n",
    "APP_NAME = \"Sentiment Analysis with Amazon Reviews Exercise\"\n",
    "SPARK_URL = \"local[*]\"\n",
    "RANDOM_SEED = 1989\n",
    "TRAINING_DATA_RATIO = 0.8\n",
    "RFC_TREES = 10\n",
    "TFIDF_FEAT = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Spark Context\n",
    "sc = SparkSession.builder.appName(APP_NAME).master(SPARK_URL).getOrCreate()\n",
    "\n",
    "# SQL context for the Spark session\n",
    "sqlcontext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "df = sqlcontext.read.json(DATA_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previewing the data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('asin', 'string'), ('helpful', 'array<bigint>'), ('overall', 'double'), ('reviewText', 'string'), ('reviewTime', 'string'), ('reviewerID', 'string'), ('reviewerName', 'string'), ('summary', 'string'), ('unixReviewTime', 'bigint')]\n",
      "+----------+--------+-------+--------------------+-----------+--------------+--------------------+--------------------+--------------+\n",
      "|      asin| helpful|overall|          reviewText| reviewTime|    reviewerID|        reviewerName|             summary|unixReviewTime|\n",
      "+----------+--------+-------+--------------------+-----------+--------------+--------------------+--------------------+--------------+\n",
      "|1384719342|  [0, 0]|    5.0|Not much to write...|02 28, 2014|A2IBPI20UZIR0U|cassandra tu \"Yea...|                good|    1393545600|\n",
      "|1384719342|[13, 14]|    5.0|The product does ...|03 16, 2013|A14VAT5EAX3D9S|                Jake|                Jake|    1363392000|\n",
      "|1384719342|  [1, 1]|    5.0|The primary job o...|08 28, 2013|A195EZSQDW3E21|Rick Bennette \"Ri...|It Does The Job Well|    1377648000|\n",
      "|1384719342|  [0, 0]|    5.0|Nice windscreen p...|02 14, 2014|A2C00NNG1ZQQG2|RustyBill \"Sunday...|GOOD WINDSCREEN F...|    1392336000|\n",
      "|1384719342|  [0, 0]|    5.0|This pop filter i...|02 21, 2014| A94QU4C90B1AX|       SEAN MASLANKA|No more pops when...|    1392940800|\n",
      "+----------+--------+-------+--------------------+-----------+--------------+--------------------+--------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df.dtypes)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- asin: string (nullable = true)\n",
      " |-- helpful: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- overall: double (nullable = true)\n",
      " |-- reviewText: string (nullable = true)\n",
      " |-- reviewTime: string (nullable = true)\n",
      " |-- reviewerID: string (nullable = true)\n",
      " |-- reviewerName: string (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- unixReviewTime: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's combine the summary and overall columns to generate one text column that we can perform NLP and subsequent modeling on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fullText = concat(col(\"reviewText\"), lit(\" \"), col(\"summary\"))\n",
    "df = df.withColumn(\"fullText\",fullText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['asin',\n",
       " 'helpful',\n",
       " 'overall',\n",
       " 'reviewText',\n",
       " 'reviewTime',\n",
       " 'reviewerID',\n",
       " 'reviewerName',\n",
       " 'summary',\n",
       " 'unixReviewTime',\n",
       " 'fullText']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we should define what a \"good\" and \"bad\" review are. The star ratings are contained within the 'overall' column on a scale of 0 to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|overall|count|\n",
      "+-------+-----+\n",
      "|    1.0|  217|\n",
      "|    4.0| 2084|\n",
      "|    3.0|  772|\n",
      "|    2.0|  250|\n",
      "|    5.0| 6938|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('overall').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(overall=1.0, count=217),\n",
       " Row(overall=4.0, count=2084),\n",
       " Row(overall=3.0, count=772),\n",
       " Row(overall=2.0, count=250),\n",
       " Row(overall=5.0, count=6938)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupBy('overall').count().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6761524217912485"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupBy('overall').count().collect()[4][1]/df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we have some pretty severe class imbalance, with high reviews being much more common. Let's just use whether a review is 5 (about 2/3 of observations) vs. whether it's not as a measure of \"good\" or \"bad.\" This is imperfect, but will make it a more straightforward problem to model.\n",
    "\n",
    "We'll need to first create a new column that captures whether each review is good or bad based on this criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "|good|overall|\n",
      "+----+-------+\n",
      "|   1|    5.0|\n",
      "|   1|    5.0|\n",
      "|   1|    5.0|\n",
      "|   1|    5.0|\n",
      "|   1|    5.0|\n",
      "|   1|    5.0|\n",
      "|   1|    5.0|\n",
      "|   0|    3.0|\n",
      "|   1|    5.0|\n",
      "|   1|    5.0|\n",
      "+----+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('good', when(col('overall') == '5', 1).otherwise(0)).select('good','overall').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('good', when(col('overall') == '5', 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['asin',\n",
       " 'helpful',\n",
       " 'overall',\n",
       " 'reviewText',\n",
       " 'reviewTime',\n",
       " 'reviewerID',\n",
       " 'reviewerName',\n",
       " 'summary',\n",
       " 'unixReviewTime',\n",
       " 'fullText',\n",
       " 'good']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's vectorize our data using TFIDF. A word2vec solution might be preferable if we could import a fully pretrained model, but I suspect this method would be suboptimal if trained solely on our data. So let's stick with the simpler solution for now, TFIDF.\n",
    "\n",
    "There's a useful example of this in the [Spark documentation](https://spark.apache.org/docs/2.2.0/ml-features.html#tf-idf)\n",
    "\n",
    "NOTE: I'm vectorizing the entire dataset together, instead of separating out train and test. This is technically not the right way to do it, but I can't figure out how to do the TFIDF train and then transform on the test set. To discuss w/ Vincent..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(words=['i', 'used', 'this', 'on', 'my', 'guitar', 'tone', 'and', 'volume', 'pots', 'and', 'it', 'clears', 'up', 'the', 'crackle.feels', 'a', 'little', 'oily,', 'but', 'gets', 'the', 'job', 'done', 'and', 'you', \"don't\", 'have', 'to', 'replacethe', 'potentiometers.', 'works', 'on', 'potentiometers'], avg(overall)=5.0, avg(unixReviewTime)=1306454400.0, avg(good)=1.0),\n",
       " Row(words=['quick', 'snap', 'on', 'and', 'off.', 'sturdy.', '', 'if', 'it', 'is', 'not', 'convenient,', 'it', \"won't\", 'be', 'used.', '', 'in', 'this', 'case,', 'it', 'is', 'very', 'convenient', 'and', 'easy', 'to', 'use,', 'hence,', 'your', 'prized', 'guitar', 'will', 'not', 'drop', 'to', 'the', 'floor', 'by', 'accident.', '', 'well', 'worth', 'the', 'money', 'spent!', \"don't\", 'let', 'your', 'guitar', 'fall'], avg(overall)=5.0, avg(unixReviewTime)=1358726400.0, avg(good)=1.0)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This part just provides a new column with an array for each word in the sentence of each\n",
    "tokenizer = Tokenizer(inputCol=\"fullText\", outputCol=\"words\")\n",
    "# This creates a new DF with an added column for the words separated out\n",
    "wordsData = tokenizer.transform(df)\n",
    "\n",
    "wordsData.groupby('words').mean().collect()[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(rawFeatures=SparseVector(1000, {36: 1.0, 48: 2.0, 83: 1.0, 90: 1.0, 157: 2.0, 237: 1.0, 281: 1.0, 299: 1.0, 329: 1.0, 330: 1.0, 333: 3.0, 338: 1.0, 365: 1.0, 373: 1.0, 388: 1.0, 402: 1.0, 403: 1.0, 420: 1.0, 450: 1.0, 461: 1.0, 489: 1.0, 493: 1.0, 527: 1.0, 646: 1.0, 656: 1.0, 710: 3.0, 725: 2.0, 765: 1.0, 770: 1.0, 839: 1.0, 860: 1.0, 906: 3.0, 916: 1.0, 956: 1.0}), avg(overall)=5.0, avg(unixReviewTime)=1373846400.0, avg(good)=1.0),\n",
       " Row(rawFeatures=SparseVector(1000, {13: 1.0, 44: 1.0, 55: 1.0, 82: 1.0, 84: 2.0, 133: 1.0, 170: 1.0, 187: 2.0, 197: 1.0, 210: 1.0, 248: 1.0, 253: 1.0, 260: 1.0, 276: 1.0, 281: 2.0, 333: 1.0, 343: 1.0, 368: 1.0, 372: 1.0, 373: 2.0, 388: 2.0, 425: 1.0, 471: 1.0, 495: 3.0, 497: 1.0, 504: 1.0, 527: 1.0, 533: 1.0, 543: 1.0, 594: 1.0, 600: 1.0, 650: 2.0, 665: 1.0, 692: 2.0, 703: 1.0, 710: 2.0, 744: 1.0, 763: 1.0, 823: 1.0, 904: 1.0, 942: 1.0, 959: 1.0, 964: 1.0, 986: 1.0}), avg(overall)=5.0, avg(unixReviewTime)=1380758400.0, avg(good)=1.0)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From the documentation:\n",
    "# While applying HashingTF only needs a single pass to the data, applying IDF needs two passes:\n",
    "# First to compute the IDF vector and second to scale the term frequencies by IDF.\n",
    "\n",
    "# First pass, which I think is just the \"frequency\" part...\n",
    "# I'm limiting the numfeatures just to keep things running (I was getting errors later on)\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\",numFeatures=TFIDF_FEAT)\n",
    "# This then transforms the previous dataframe into a new one with the rawFeatures column\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "\n",
    "featurizedData.groupby('rawFeatures').mean().collect()[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features=SparseVector(1000, {11: 11.6673, 18: 1.0032, 28: 3.8113, 36: 0.8437, 76: 1.618, 82: 1.5393, 83: 1.5712, 95: 3.8891, 157: 1.3337, 170: 1.5813, 183: 2.4427, 187: 1.3118, 189: 3.0645, 193: 2.7334, 213: 1.6378, 223: 2.235, 234: 1.409, 237: 2.3067, 248: 2.5064, 260: 1.068, 263: 1.5051, 281: 3.7149, 284: 2.5731, 299: 1.7631, 309: 3.3013, 310: 1.1198, 313: 2.5503, 320: 4.6271, 328: 2.9373, 329: 1.0906, 333: 1.2335, 335: 2.6201, 341: 3.3811, 343: 1.2142, 346: 1.6165, 347: 3.6014, 348: 2.8715, 352: 2.3136, 364: 3.1771, 368: 2.2618, 372: 3.605, 373: 1.5462, 375: 2.3967, 384: 7.7405, 388: 1.1818, 400: 1.5662, 421: 2.1053, 431: 2.8871, 447: 3.0357, 448: 4.4404, 471: 2.0129, 489: 1.366, 493: 2.4173, 494: 2.7156, 495: 0.9216, 521: 3.6679, 524: 3.7119, 526: 1.5432, 541: 3.6794, 547: 2.4735, 553: 2.7215, 564: 2.235, 572: 1.1535, 590: 2.5478, 592: 3.8939, 597: 1.9337, 637: 8.9878, 650: 0.7542, 656: 1.2022, 666: 3.8327, 675: 2.0006, 690: 4.5765, 691: 2.2645, 710: 2.7993, 718: 2.9483, 723: 4.474, 732: 2.5629, 737: 2.1387, 749: 4.0213, 750: 7.2172, 754: 3.7188, 756: 1.4451, 763: 2.1843, 767: 3.8471, 777: 3.6871, 791: 3.5458, 822: 4.0432, 831: 1.9799, 843: 2.5014, 844: 6.3824, 853: 4.8542, 856: 3.5458, 872: 3.2152, 898: 3.2298, 899: 1.8144, 909: 2.9577, 921: 1.5756, 924: 1.7734, 930: 4.6512, 944: 1.2348, 953: 3.2498, 955: 7.3451, 958: 6.8742, 959: 1.5008, 979: 3.5324, 989: 2.0577, 996: 2.7905}), avg(overall)=3.0, avg(unixReviewTime)=1387238400.0, avg(good)=0.0),\n",
       " Row(features=SparseVector(1000, {36: 0.4219, 77: 1.7826, 82: 0.7697, 84: 3.6336, 187: 1.3118, 260: 3.204, 263: 4.5154, 299: 0.8815, 310: 1.1198, 340: 2.3861, 342: 2.7112, 372: 0.9013, 385: 3.0377, 388: 0.2954, 423: 3.7766, 425: 0.9275, 445: 0.7491, 787: 2.9245, 888: 2.5503, 959: 1.5008}), avg(overall)=5.0, avg(unixReviewTime)=1390694400.0, avg(good)=1.0)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Second pass, which does the inverse document frequency calcs (idf)...\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "# For some reason this needs to be fit, unlike the hashing mechanism\n",
    "idfModel = idf.fit(featurizedData)\n",
    "# Then, again, just creating a new df that's same as the previous one + the added tfidf column\n",
    "# \"Rescaled\" in this case just means rescaled down from just bag of words (document frequency)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "rescaledData.groupby('features').mean().collect()[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['asin',\n",
       " 'helpful',\n",
       " 'overall',\n",
       " 'reviewText',\n",
       " 'reviewTime',\n",
       " 'reviewerID',\n",
       " 'reviewerName',\n",
       " 'summary',\n",
       " 'unixReviewTime',\n",
       " 'fullText',\n",
       " 'good',\n",
       " 'words',\n",
       " 'rawFeatures',\n",
       " 'features']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rescaledData.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sweet. The features column in our rescaled dataset is what we'll train our model on. Let's start with a simple RFC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Label indexing\n",
    "labelIndexer = StringIndexer(inputCol='good', outputCol=\"indexedLabel\").fit(rescaledData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# now generate the indexed feature vector\n",
    "featureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\",maxCategories=4).fit(rescaledData)\n",
    "\n",
    "# Split the data into training and validation sets (20% held out for testing)\n",
    "(trainingData, testData) = rescaledData.randomSplit([TRAINING_DATA_RATIO, 1 - TRAINING_DATA_RATIO])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"features\", numTrees=RFC_TREES)\n",
    "\n",
    "# Chain indexers and forest in a Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! Now let's try to predict on the test set and evaluate how well it performs. The first step in that operation is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "predictions = model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.317933\n",
      "Accuracy = 0.682067\n"
     ]
    }
   ],
   "source": [
    "# eval\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"Test Error = {(1.0 - accuracy):g}\")\n",
    "print(f\"Accuracy = {accuracy:g}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance is pretty bad, considering that that's close to baseline accuracy, but I guess that's beside the point! Let's try logit regression just to experiment with how that works.\n",
    "\n",
    "__NOTE: LRC doesn't seem to need to require labeled indexes for the inputs__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lrc = LogisticRegression(labelCol=\"good\", featuresCol=\"features\",maxIter=10, regParam=0.5, elasticNetParam=0.8)\n",
    "lrcmodel = lrc.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict - note that this creates a new column called predictions in the test dataset\n",
    "predictions = lrcmodel.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['asin',\n",
       " 'helpful',\n",
       " 'overall',\n",
       " 'reviewText',\n",
       " 'reviewTime',\n",
       " 'reviewerID',\n",
       " 'reviewerName',\n",
       " 'summary',\n",
       " 'unixReviewTime',\n",
       " 'fullText',\n",
       " 'good',\n",
       " 'words',\n",
       " 'rawFeatures',\n",
       " 'features',\n",
       " 'rawPrediction',\n",
       " 'probability',\n",
       " 'prediction']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.320417\n",
      "Accuracy = 0.679583\n"
     ]
    }
   ],
   "source": [
    "# eval\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"good\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"Test Error = {(1.0 - accuracy):g}\")\n",
    "print(f\"Accuracy = {accuracy:g}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not very good either! Oh well. Keep in mind that we're only using 1000 words, and didn't do any LSA, so it's to be expected that this didn't perform all that great."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work with Vincent on this: how would I get the model to just TFIDF vectorize on the training set, and then just apply that vectorization to the test set? Pyspark does not seem to have the same distinctions between transform and fit_transform that we rely on in sklearn\n",
    "\n",
    "The start of what I would do..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainWordsData' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-ea9ee62bb857>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# TFIDF for just the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mhashingTF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHashingTF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"words\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"rawFeatures\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnumFeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrainFeaturizedData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhashingTF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainWordsData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtrainFeaturizedData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rawFeatures'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0midf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"rawFeatures\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"features\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainWordsData' is not defined"
     ]
    }
   ],
   "source": [
    "# TFIDF for just the \n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\",numFeatures=20)\n",
    "trainFeaturizedData = hashingTF.transform(trainWordsData)\n",
    "trainFeaturizedData.groupby('rawFeatures').mean().collect()[0:2]\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(trainFeaturizedData)\n",
    "trainRescaledData = idfModel.transform(trainFeaturizedData)\n",
    "testWordsData = tokenizer.transform(testData)\n",
    "\n",
    "# How do I do this part?\n",
    "testFeaturizedData = hashingTF.transform(testWordsData)\n",
    "testFeaturizedData.groupby('rawFeatures').mean().collect()[0:2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
