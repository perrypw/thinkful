{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utility function for standard text cleaning.\n",
    "def text_cleaner(text):\n",
    "    # Visual inspection identifies a form of punctuation spaCy does not\n",
    "    # recognize: the double dash '--'.  Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "    \n",
    "# Load and clean the data.\n",
    "persuasion = gutenberg.raw('austen-persuasion.txt')\n",
    "alice = gutenberg.raw('carroll-alice.txt')\n",
    "\n",
    "# The Chapter indicator is idiosyncratic\n",
    "persuasion = re.sub(r'Chapter \\d+', '', persuasion)\n",
    "alice = re.sub(r'CHAPTER .*', '', alice)\n",
    "    \n",
    "alice = text_cleaner(alice)\n",
    "persuasion = text_cleaner(persuasion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parse the cleaned novels. This can take a bit.\n",
    "nlp = spacy.load('en')\n",
    "alice_doc = nlp(alice)\n",
    "persuasion_doc = nlp(persuasion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(So, she, was, considering, in, her, own, mind...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Oh, dear, !)</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(I, shall, be, late, !, ')</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0        1\n",
       "0  (Alice, was, beginning, to, get, very, tired, ...  Carroll\n",
       "1  (So, she, was, considering, in, her, own, mind...  Carroll\n",
       "2  (There, was, nothing, so, VERY, remarkable, in...  Carroll\n",
       "3                                      (Oh, dear, !)  Carroll\n",
       "4                         (I, shall, be, late, !, ')  Carroll"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group into sentences. # the second col is just an indicator for the book that it is included in\n",
    "alice_sents = [[sent, \"Carroll\"] for sent in alice_doc.sents]\n",
    "persuasion_sents = [[sent, \"Austen\"] for sent in persuasion_doc.sents]\n",
    "\n",
    "# Combine the sentences from the two novels into one data frame.\n",
    "sentences = pd.DataFrame(alice_sents + persuasion_sents)\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utility function to create a list of the 2000 most common words, minus stops and punctuation\n",
    "def bag_of_words(text):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    allwords = [token.lhamlet_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(1000)]\n",
    "    \n",
    "\n",
    "# Creates a data frame with features for each word in our common word set.\n",
    "# Each value is the count of the times the word appears in each sentence.\n",
    "def bow_features(sentences, common_words):\n",
    "    \n",
    "    # Scaffold the data frame and initialize counts to zero.\n",
    "    # NOTE: the columns 0 and 1 referenced below are not the 1st and second sentences, they are\n",
    "    # REFERENCES to the 0 and 1 columns in the DF with the sentences\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    # Setting these all equal to zero\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "        # Convert the sentence to lhamlets, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.lhamlet_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lhamlet_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 100 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return df\n",
    "\n",
    "# Set up the bags.\n",
    "alicewords = bag_of_words(alice_doc)\n",
    "persuasionwords = bag_of_words(persuasion_doc)\n",
    "\n",
    "# Combine bags to create a set of unique words.\n",
    "common_words = set(alicewords + persuasionwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 100\n",
      "Processing row 200\n",
      "Processing row 300\n",
      "Processing row 400\n",
      "Processing row 500\n",
      "Processing row 600\n",
      "Processing row 700\n",
      "Processing row 800\n",
      "Processing row 900\n",
      "Processing row 1000\n",
      "Processing row 1100\n",
      "Processing row 1200\n",
      "Processing row 1300\n",
      "Processing row 1400\n",
      "Processing row 1500\n",
      "Processing row 1600\n",
      "Processing row 1700\n",
      "Processing row 1800\n",
      "Processing row 1900\n",
      "Processing row 2000\n",
      "Processing row 2100\n",
      "Processing row 2200\n",
      "Processing row 2300\n",
      "Processing row 2400\n",
      "Processing row 2500\n",
      "Processing row 2600\n",
      "Processing row 2700\n",
      "Processing row 2800\n",
      "Processing row 2900\n",
      "Processing row 3000\n",
      "Processing row 3100\n",
      "Processing row 3200\n",
      "Processing row 3300\n",
      "Processing row 3400\n",
      "Processing row 3500\n",
      "Processing row 3600\n",
      "Processing row 3700\n",
      "Processing row 3800\n",
      "Processing row 3900\n",
      "Processing row 4000\n",
      "Processing row 4100\n",
      "Processing row 4200\n",
      "Processing row 4300\n",
      "Processing row 4400\n",
      "Processing row 4500\n",
      "Processing row 4600\n",
      "Processing row 4700\n",
      "Processing row 4800\n",
      "Processing row 4900\n",
      "Processing row 5000\n",
      "Processing row 5100\n",
      "Processing row 5200\n",
      "Processing row 5300\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stay</th>\n",
       "      <th>reflection</th>\n",
       "      <th>wretched</th>\n",
       "      <th>mock</th>\n",
       "      <th>shew</th>\n",
       "      <th>prospect</th>\n",
       "      <th>welcome</th>\n",
       "      <th>leaf</th>\n",
       "      <th>street</th>\n",
       "      <th>recover</th>\n",
       "      <th>...</th>\n",
       "      <th>golden</th>\n",
       "      <th>comfort</th>\n",
       "      <th>indulgence</th>\n",
       "      <th>the</th>\n",
       "      <th>number</th>\n",
       "      <th>bread</th>\n",
       "      <th>pack</th>\n",
       "      <th>opinion</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(So, she, was, considering, in, her, own, mind...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Oh, dear, !)</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(I, shall, be, late, !, ')</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1563 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  stay reflection wretched mock shew prospect welcome leaf street recover  \\\n",
       "0    0          0        0    0    0        0       0    0      0       0   \n",
       "1    0          0        0    0    0        0       0    0      0       0   \n",
       "2    0          0        0    0    0        0       0    0      0       0   \n",
       "3    0          0        0    0    0        0       0    0      0       0   \n",
       "4    0          0        0    0    0        0       0    0      0       0   \n",
       "\n",
       "      ...     golden comfort indulgence the number bread pack opinion  \\\n",
       "0     ...          0       0          0   0      0     0    0       0   \n",
       "1     ...          0       0          0   0      0     0    0       0   \n",
       "2     ...          0       0          0   0      0     0    0       0   \n",
       "3     ...          0       0          0   0      0     0    0       0   \n",
       "4     ...          0       0          0   0      0     0    0       0   \n",
       "\n",
       "                                       text_sentence text_source  \n",
       "0  (Alice, was, beginning, to, get, very, tired, ...     Carroll  \n",
       "1  (So, she, was, considering, in, her, own, mind...     Carroll  \n",
       "2  (There, was, nothing, so, VERY, remarkable, in...     Carroll  \n",
       "3                                      (Oh, dear, !)     Carroll  \n",
       "4                         (I, shall, be, late, !, ')     Carroll  \n",
       "\n",
       "[5 rows x 1563 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create our data frame with features. This can take a while to run.\n",
    "hamlet_bow = bow_features(sentences, common_words)\n",
    "hamlet_bow.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now time to model...\n",
    "\n",
    "__Random Forest Classifier__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.987460815047\n",
      "\n",
      "Test set score: 0.888157894737\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "rfc = ensemble.RandomForestClassifier()\n",
    "Y = hamlet_bow['text_source']\n",
    "X = np.array(hamlet_bow.drop(['text_sentence','text_source'], 1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    Y,\n",
    "                                                    test_size=0.4,\n",
    "                                                    random_state=0)\n",
    "train = rfc.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', rfc.score(X_train, y_train))\n",
    "print('\\nTest set score:', rfc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Logistic Regression__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3190, 1561) (3190,)\n",
      "Training set score: 0.952351097179\n",
      "\n",
      "Test set score: 0.917763157895\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "train = lr.fit(X_train, y_train)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Gradient Boosted Classifier__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.886833855799\n",
      "\n",
      "Test set score: 0.874060150376\n"
     ]
    }
   ],
   "source": [
    "clf = ensemble.GradientBoostingClassifier()\n",
    "train = clf.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', clf.score(X_train, y_train))\n",
    "print('\\nTest set score:', clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try a different novel by Jane Austen, *hamlet*, and see if it can be distinguished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emma Woodhouse, handsome, clever, and rich, with a comfortable home and happy disposition, seemed to\n"
     ]
    }
   ],
   "source": [
    "# Clean the Emma data.\n",
    "emma = gutenberg.raw('austen-emma.txt')\n",
    "emma = re.sub(r'VOLUME \\w+', '', emma)\n",
    "emma = re.sub(r'CHAPTER \\w+', '', emma)\n",
    "emma = text_cleaner(emma)\n",
    "print(emma[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parse our cleaned data.\n",
    "emma_doc = nlp(emma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Group into sentences.\n",
    "persuasion_sents = [[sent, \"Austen\"] for sent in persuasion_doc.sents]\n",
    "emma_sents = [[sent, \"Austen\"] for sent in emma_doc.sents]\n",
    "\n",
    "# Emma is quite long, let's cut it down to the same length as Alice.\n",
    "emma_sents = emma_sents[0:len(alice_sents)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 100\n",
      "Processing row 200\n",
      "Processing row 300\n",
      "Processing row 400\n",
      "Processing row 500\n",
      "Processing row 600\n",
      "Processing row 700\n",
      "Processing row 800\n",
      "Processing row 900\n",
      "Processing row 1000\n",
      "Processing row 1100\n",
      "Processing row 1200\n",
      "Processing row 1300\n",
      "Processing row 1400\n",
      "Processing row 1500\n",
      "Processing row 1600\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# Build a new Bag of Words data frame for Emma word counts.\n",
    "# We'll use the same common words from Alice and Persuasion.\n",
    "emma_sentences = pd.DataFrame(emma_sents)\n",
    "emma_bow = bow_features(emma_sentences, common_words)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set score: 0.693512304251\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>Austen</th>\n",
       "      <th>Carroll</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Austen</th>\n",
       "      <td>1558</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Carroll</th>\n",
       "      <td>711</td>\n",
       "      <td>302</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0    Austen  Carroll\n",
       "row_0                   \n",
       "Austen     1558      111\n",
       "Carroll     711      302"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we can model it!\n",
    "# Let's use logistic regression again.\n",
    "\n",
    "# Combine the Emma sentence data with the Alice data from the test set.\n",
    "X_Emma_test = np.concatenate((\n",
    "    X_train[y_train[y_train=='Carroll'].index],\n",
    "    emma_bow.drop(['text_sentence','text_source'], 1)\n",
    "), axis=0)\n",
    "y_Emma_test = pd.concat([y_train[y_train=='Carroll'],\n",
    "                         pd.Series(['Austen'] * emma_bow.shape[0])])\n",
    "\n",
    "# Model.\n",
    "print('\\nTest set score:', lr.score(X_Emma_test, y_Emma_test))\n",
    "lr_Emma_predicted = lr.predict(X_Emma_test)\n",
    "pd.crosstab(y_Emma_test, lr_Emma_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad, so let's do a couple challenges...\n",
    "\n",
    "# Challenge 1:\n",
    "Improve the model's performance on the test set. Let's mess with SVM for starters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3190, 1561) (3190,)\n",
      "Training set score: 0.685893416928\n",
      "\n",
      "Test set score: 0.692669172932\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "svc = svm.SVC()\n",
    "\n",
    "train = svc.fit(X_train, y_train)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print('Training set score:', svc.score(X_train, y_train))\n",
    "print('\\nTest set score:', svc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doesn't look like SVM is likely to improve our results much, so let's instead optimize our logreg through gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'penalty':['l1','l2'], 'C':[0.9,0.95,1,2,3,4,5,10,100,1000,10000]}\n",
    "\n",
    "clf = GridSearchCV(lr,parameters)\n",
    "clf.fit(X,Y)\n",
    "best_params = clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.95, 'penalty': 'l2'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3190, 1561) (3190,)\n",
      "Training set score: 0.951410658307\n",
      "\n",
      "Test set score: 0.918233082707\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(**best_params)\n",
    "train = lr.fit(X_train, y_train)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gets us a very modest improvement, which is something!\n",
    "\n",
    "# Challenge 2\n",
    "See if our new model is any good at predicting a new novel vs. Alice in Wonderland or Austen vs. another work.\n",
    "\n",
    "Let's pick something else from Gutenberg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actus Primus. Scoena Prima. Enter Barnardo and Francisco two Centinels. Barnardo. Who's there? Fran.\n"
     ]
    }
   ],
   "source": [
    "# Clean the hamlet data.\n",
    "hamlet = gutenberg.raw('shakespeare-hamlet.txt')\n",
    "hamlet = re.sub(r'VOLUME \\w+', '', hamlet)\n",
    "hamlet = re.sub(r'CHAPTER \\w+', '', hamlet)\n",
    "hamlet = text_cleaner(hamlet)\n",
    "print(hamlet[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parse our cleaned data.\n",
    "hamlet_doc = nlp(hamlet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Group into sentences.\n",
    "persuasion_sents = [[sent, \"Shakespeare\"] for sent in persuasion_doc.sents]\n",
    "hamlet_sents = [[sent, \"Shakespeare\"] for sent in hamlet_doc.sents]\n",
    "\n",
    "# hamlet is also long, so let's cut sentences down to length of alice\n",
    "hamlet_sents = hamlet_sents[0:len(alice_sents)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 100\n",
      "Processing row 200\n",
      "Processing row 300\n",
      "Processing row 400\n",
      "Processing row 500\n",
      "Processing row 600\n",
      "Processing row 700\n",
      "Processing row 800\n",
      "Processing row 900\n",
      "Processing row 1000\n",
      "Processing row 1100\n",
      "Processing row 1200\n",
      "Processing row 1300\n",
      "Processing row 1400\n",
      "Processing row 1500\n",
      "Processing row 1600\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# Build a new Bag of Words data frame for hamlet word counts.\n",
    "# We'll use the same common words from Alice and Persuasion.\n",
    "hamlet_sentences = pd.DataFrame(hamlet_sents)\n",
    "hamlet_bow = bow_features(hamlet_sentences, common_words)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set score: 0.112229679344\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>Austen</th>\n",
       "      <th>Carroll</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Carroll</th>\n",
       "      <td>712</td>\n",
       "      <td>301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Shakespeare</th>\n",
       "      <td>1390</td>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0        Austen  Carroll\n",
       "row_0                       \n",
       "Carroll         712      301\n",
       "Shakespeare    1390      279"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modeling again using logistic regression\n",
    "X_hamlet_test = np.concatenate((\n",
    "    X_train[y_train[y_train=='Carroll'].index],\n",
    "    hamlet_bow.drop(['text_sentence','text_source'], 1)\n",
    "), axis=0)\n",
    "y_hamlet_test = pd.concat([y_train[y_train=='Carroll'],\n",
    "                         pd.Series(['Shakespeare'] * hamlet_bow.shape[0])])\n",
    "\n",
    "# Model.\n",
    "print('\\nTest set score:', lr.score(X_hamlet_test, y_hamlet_test))\n",
    "lr_hamlet_predicted = lr.predict(X_hamlet_test)\n",
    "pd.crosstab(y_hamlet_test, lr_hamlet_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model didn't do a great job with this new data, mischaracterizing ~70% (712/1013) instances of the Carroll novel, but this is a bit unfair. Let's see how it does when refit to actual Shakespeare data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "carroll = word_counts[word_counts['text_source'] == 'Carroll']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.concatenate((carroll.drop(['text_sentence','text_source'], 1),\n",
    "    hamlet_bow.drop(['text_sentence','text_source'], 1)))\n",
    "Y = np.concatenate((carroll['text_source'],\n",
    "    hamlet_bow['text_source']),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y,test_size=0.4,random_state=0)\n",
    "lr = LogisticRegression(**best_params)\n",
    "train = lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2002, 1561) (2002,)\n",
      "Training set score: 0.929070929071\n",
      "\n",
      "Test set score: 0.86377245509\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape)\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model does pretty good once it's refit to the new data, even if the original model doesn't do so well when just trying to identify Carroll."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
