{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each, identify which supervised learning method(s) would be best for addressing that particular problem. Explain your reasoning and discuss your answers with your mentor.\n",
    "<br>\n",
    "__1. Predict the running times of prospective Olympic sprinters using data from the last 20 Olympics__\n",
    "\n",
    "Since we're interested in predicting a continuous outcome variable for the data, we'll want to use some type of regression model. Which one will end up being the best choice will depend heavily on the variables made available to us for use as features. However, if the available features are things like year, time per seed, and other basic metrics, I suspect a simple multivariate regression will work well.\n",
    "\n",
    "Note: falls into general category of time series forecasting (where a trend over time exists)\n",
    "\n",
    "__2. You have more features (columns) than rows in your dataset.__\n",
    "\n",
    "Again, the best option will depend on the specifics of the situation. If the number of fields is still relatively small, you may want to try to reduce the feature set to prevent overfitting. Options for this include PCA or partial least squares regression (for continuous variables) and various filtering methods for feature selection (categorical or continuous). If the number of variables is too large or unwieldy to realistically sift through all of the variables, lasso and ridge regression could be of high value given that they involve feature weighting/selection as part of the model fit. Lastly, a random forest, gradient boosted model, or SVM could work well with a large feature set, but may cause performance issues depending on the size of the feature set.\n",
    "\n",
    "Note: regularization of some sort is required, given that if a dataset has more features than observations, there is always a model that can be fit that has 100% accuracy on the training set (ie is severely overfit). When the errors are huge for individual trees, even random forests are susceptible to big problems.\n",
    "\n",
    "__3. Identify the most important characteristic predicting likelihood of being jailed before age 20.__\n",
    "\n",
    "Assuming you are looking at many different potential variables, I would recommend either a gradient boost or random forest (computational resources permitting). After the model is fit, you can run .feature_importance to determine which features were most valuable in the model assuming that it is accurate. An alternative is to perform f-tests on multivariate regression variables, but this runs the risk of being affected by multicollinarity if there are many correlated variables.\n",
    "\n",
    "__4. Implement a filter to “highlight” emails that might be important to the recipient__\n",
    "\n",
    "This is a classification problem. I think a good place to start would be a simple Naive Bayes classifier. Since keywords are *usually* independent from one another, this is a valid approach that should be straightforward to interpret while providing accurate results. This model type is also known to do well in text evaluation problems such as email filtering.\n",
    "\n",
    "__5. You have 1000+ features.__\n",
    "\n",
    "Given that it's not realistic to individually analyze the value of each of these features and their interrelations, it would be best to go with either a lasso or ridge classification/regression. This will help us do the work of weighting the important features over the unimportant ones and prevent overfitting in the process. Whether we use ridge or regression will depend on whether we suspect it would be better to include all features, with the coefficients of the less important ones minimized (ridge), or to try to get rid of less important features entirely (lasso). Regardless, it's probably worthwhile to cross-validate with both to see which produces better results on our test sets.\n",
    "\n",
    "__6. Predict whether someone who adds items to their cart on a website will purchase the items.__\n",
    "\n",
    "I think a KNN model would work best here, the reason being that there are a lot of potential factors in whether or not the purchase will be made, probably more than would be worthwhile to investigate the predictive value of. We're more interested in finding out whether a person with very specific characteristics will perform an action, in which case it is probably the most efficient to just identify the people who also had the same items in their cart and take a weighted average \"vote\" to make our prediction of what the customer will do.\n",
    "\n",
    "__7. Your dataset dimensions are 982400 x 500__\n",
    "\n",
    "Assuming you want to keep them all, you likely have too many features for KNN, logistic/multivar regression, and Naive Bayes (if you suspect some variable dependence). Random forest, gradient boosting, and SVM might be too computationally intensive. As such, I would recommend going with lasso or ridge regression/classification. Another option is to reduce the features through the previously mentioned methods and go from there.\n",
    "\n",
    "Note: dataset is really large - you may need to use a sample, or use a model that doesn't require all of the data to be in memory simultaneously, such as stochastic gradient descent or naive bayes or KNN (in which you don't need to review all observation coordinates at the same time).\n",
    "\n",
    "__8. Identify faces in an image.__\n",
    "\n",
    "Assuming the data can be vectorized into features summarizing different blocs of the image, I think a support vector machine might work well here. This is because it can capture complex relationships between your variables (ones which presumably would exist in successfully identifying a face). Logistic regression and KNN, while potentially worth trying, may struggle with these complex relationships and be prone to overfitting.\n",
    "\n",
    "Note: you are looking for patterns in pixels and probably not the values of specific pixels in isolation. PCA has been shown to be relatively effective since it can capture the relationships between pixels (patterns). The important thing is that feature engineering is required, since any model based solely on individual pixel values will be inaccurate.\n",
    "\n",
    "__9. Predict which of three flavors of ice cream will be most popular with boys vs girls.__\n",
    "\n",
    "Assuming you have past data, this may just involve taking a crosstab of the ice cream favorites by gender and not require any modeling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
